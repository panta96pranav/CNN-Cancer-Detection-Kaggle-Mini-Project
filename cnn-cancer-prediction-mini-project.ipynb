{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Project Topic and Goal\n\nThis project is about developing an algorithm to identify metastatic cancer in small image patches extracted from larger digital pathology scans. The dataset used for this competition is a modified version of the PatchCamelyon (PCam) benchmark dataset, which focuses on the detection of metastasis in cancer patients.\n\nThe primary problem that this project aims to solve is the automated identification of metastatic cancer in digital pathology images. This will be done by creating deep learning models that can accurately classify whether a given image patch contains evidence of cancer metastasis or not. The dataset provided for this competition simulates this task by presenting binary image classification challenges similar to well-known benchmark datasets like CIFAR-10 and MNIST.\n\nThe submissions are evaluated on the area under the ROC curve between the predicted probability and the observed target.\n\n**Github Link: https://github.com/panta96pranav/CNN-Cancer-Detection-Kaggle-Mini-Project**","metadata":{}},{"cell_type":"markdown","source":"# 2. Data\n\nThe dataset consists of many small pathology images for classification. Each image has an image id, and the train_labels.csv file offers the true labels for images in the train folder. The objective is to predict labels for images in the test folder. Positive labels indicate the presence of tumor tissue in the central 32x32px section of a patch (with tumor tissue in the surrounding area not affecting the label). This outer region supports fully-convolutional models that maintain consistency when applied to whole-slide images, without zero-padding.\n\nWhile the original PCam dataset included duplicate images due to random sampling, the version used in this competition has no duplicates. The split ratio of train and test data remains consistent with the PCam benchmark.\n\nTrain and test data combined exhibit a size of rougly 7.76 GB which is quite large. There are 220,000 training train images and 57,000 test images, each represented in a .tif format. TIFF (Tagged Image File Format) is a versatile image file format used for various purposes, including photography or medical imaging. Even compressed it maintains high image quality which leads to larger file sizes that can impact the required storage size compared to formats like JPEG or PNG.\n\nFor evaluation we are required to create a submission file consisting of a 2-column table. First, there will be the ID of the test images and second the predicted label (1 = positive, 0 = negative).","metadata":{}},{"cell_type":"markdown","source":"# 3. Import Python Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport gc\n\nfrom PIL import Image\ntrain_on_gpu = True\n\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score\n\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\nimport torchvision\nimport torch.optim as optim\nimport torchvision.models as models\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:41.863775Z","iopub.execute_input":"2024-02-29T08:45:41.864176Z","iopub.status.idle":"2024-02-29T08:45:41.873904Z","shell.execute_reply.started":"2024-02-29T08:45:41.864145Z","shell.execute_reply":"2024-02-29T08:45:41.872705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# helper method for clearing GPU memory\ndef clear_memory():\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:42.149265Z","iopub.execute_input":"2024-02-29T08:45:42.150346Z","iopub.status.idle":"2024-02-29T08:45:42.155374Z","shell.execute_reply.started":"2024-02-29T08:45:42.150303Z","shell.execute_reply":"2024-02-29T08:45:42.154239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. EDA and Data Preprocessing\n\n## 4.1 Data Inspection\n\nFirst let's look at our training data by loading it into a dataframe. We also load the sample submission file which we will use later on.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"../input/histopathologic-cancer-detection/train_labels.csv\")\ndf_sample_sub = pd.read_csv(\"../input/histopathologic-cancer-detection/sample_submission.csv\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:42.157778Z","iopub.execute_input":"2024-02-29T08:45:42.158113Z","iopub.status.idle":"2024-02-29T08:45:42.495881Z","shell.execute_reply.started":"2024-02-29T08:45:42.158086Z","shell.execute_reply":"2024-02-29T08:45:42.494784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first column contains the ID of an image that is also the name of the corresponding image TIFF file. The second column shows the label where a 1 indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue. ","metadata":{}},{"cell_type":"code","source":"folder_train = \"../input/histopathologic-cancer-detection/train/\"\nfolder_test = \"../input/histopathologic-cancer-detection/test/\"\n\nprint(\"Number of training images: {}\".format(len(os.listdir(folder_train))))\nprint(\"Number of test images: {}\".format(len(os.listdir(folder_test))))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:42.497149Z","iopub.execute_input":"2024-02-29T08:45:42.497464Z","iopub.status.idle":"2024-02-29T08:45:46.616300Z","shell.execute_reply.started":"2024-02-29T08:45:42.497437Z","shell.execute_reply":"2024-02-29T08:45:46.615241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As already mentioned we are working with a really large dataset. To get an intuition what the images look like, we will print a few of them choosen randomly with the according labels.","metadata":{}},{"cell_type":"code","source":"# load the images\nimg_train = os.listdir(folder_train)\nimg_test = os.listdir(folder_test)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:46.617725Z","iopub.execute_input":"2024-02-29T08:45:46.618097Z","iopub.status.idle":"2024-02-29T08:45:46.843271Z","shell.execute_reply.started":"2024-02-29T08:45:46.618063Z","shell.execute_reply":"2024-02-29T08:45:46.842053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print the first 10 images\nfig = plt.figure(figsize=(25, 4))\nfor i in range(10):\n    ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n    im = Image.open(folder_train + img_train[i])\n    plt.imshow(im)\n    label = df_train.loc[df_train['id'] == img_train[i].split('.')[0], 'label'].values[0]\n    ax.set_title(f'#{i+1} - Label: {label}')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:46.846278Z","iopub.execute_input":"2024-02-29T08:45:46.846622Z","iopub.status.idle":"2024-02-29T08:45:48.369134Z","shell.execute_reply.started":"2024-02-29T08:45:46.846595Z","shell.execute_reply":"2024-02-29T08:45:48.368003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unfortunately, I do not have the necessary domain knowledge to explain why certain pictures are labeled as positive and why some are not. But what we can see is that even positive ones among themselves differ quite alot looking at their color and structures. Interestingly, picture 5 and 9 look really similar on the first glance but they are not labeled the same. This indicates that there can be really small image features deciding whether it is a positive or negative one and our model has to recognize those tiny differences.","metadata":{}},{"cell_type":"markdown","source":"Next, let's investigate the images in more detail by looking at their pixel values. For this purpose, we will plot the distribution of the grayscale pixel values for the first 10 positive and negative images. ","metadata":{}},{"cell_type":"code","source":"# sample first 10 images for both labels\ncancer_samples = df_train[df_train['label'] == 1].head(10)\nno_cancer_samples = df_train[df_train['label'] == 0].head(10)\n\n# plot histograms of image pixel values for cancer and no cancer images\nplt.figure(figsize=(12, 6))\nfor i in range(10):\n    plt.subplot(4, 5, i + 1)\n    cancer_img = cv2.imread(folder_train + cancer_samples.iloc[i]['id'] + '.tif', cv2.IMREAD_GRAYSCALE)\n    plt.hist(cancer_img.ravel(), bins=128, color='red', alpha=0.7)\n    plt.title(f'Cancer Sample #{i+1}')\n    \n    plt.subplot(4, 5, i + 11)\n    no_cancer_img = cv2.imread(folder_train + no_cancer_samples.iloc[i]['id'] + '.tif', cv2.IMREAD_GRAYSCALE)\n    plt.hist(no_cancer_img.ravel(), bins=128, color='blue', alpha=0.7)\n    plt.title(f'No Cancer Sample #{i+1}')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:48.370661Z","iopub.execute_input":"2024-02-29T08:45:48.371032Z","iopub.status.idle":"2024-02-29T08:45:56.455713Z","shell.execute_reply.started":"2024-02-29T08:45:48.371000Z","shell.execute_reply":"2024-02-29T08:45:56.454507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The upper two rows show the first 10 images labeled as positive and the the lower two rows show the first 10 images labeled as negative. Since we are only looking at the grayscale values we can only talk about images being darker (higher pixel values) or lighter (smaller pixel values). A plot like the one on the left in the third row (No Cancer Sample # 1) signals that the image mostly consists of one color with little contrast. The second plot in the first row however (Cancer Sample # 2) suggests that the image has higher contrast and different colors. We can verify this by looking back at our corresponding printed images that were labeled as positive respectively negative (#2 and #4).\n\nWith this information we could suggest the positive images typically have a higher contrast and exhibit more different colors. We count 8 positive images with high contrast and only 5 negative images with high contrast. But overall the number of samples is too small to draw a statistically relevant conclusion.  ","metadata":{}},{"cell_type":"markdown","source":"Now, we will have a look at the label distribution to see if we are working with a balanced dataset.","metadata":{}},{"cell_type":"code","source":"# plot histogram of label distribution\ndef plot_label_dist(df):\n    plt.figure(figsize=(6, 4))\n    plt.hist(df['label'], bins=2, edgecolor='black', alpha=0.7)\n    plt.xticks(np.arange(2), ['No Cancer', 'Cancer'])\n    plt.xlabel('Labels')\n    plt.ylabel('Count')\n    plt.title('Label Distribution')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:56.456891Z","iopub.execute_input":"2024-02-29T08:45:56.457217Z","iopub.status.idle":"2024-02-29T08:45:56.464027Z","shell.execute_reply.started":"2024-02-29T08:45:56.457190Z","shell.execute_reply":"2024-02-29T08:45:56.462944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_label_dist(df_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:56.465722Z","iopub.execute_input":"2024-02-29T08:45:56.466216Z","iopub.status.idle":"2024-02-29T08:45:56.669119Z","shell.execute_reply.started":"2024-02-29T08:45:56.466186Z","shell.execute_reply":"2024-02-29T08:45:56.668116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculate the imbalance ratios\ndef calc_imbalance(df_train):\n    \n    df_train_cancer = df_train[df_train['label'] == 1]\n    df_train_no_cancer = df_train[df_train['label'] == 0]\n    \n    cancer = len(df_train_cancer)\n    no_cancer = len(df_train_no_cancer)\n\n    imbalance_ratio = no_cancer / cancer\n    cancer_ratio = cancer / (cancer + no_cancer)\n\n    print(\"Imbalance ratio:\", round(imbalance_ratio, 3))\n    print(\"Ratio of cancer:\", round(cancer_ratio, 3))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:56.670482Z","iopub.execute_input":"2024-02-29T08:45:56.670788Z","iopub.status.idle":"2024-02-29T08:45:56.679629Z","shell.execute_reply.started":"2024-02-29T08:45:56.670762Z","shell.execute_reply":"2024-02-29T08:45:56.678693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_imbalance(df_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:56.680873Z","iopub.execute_input":"2024-02-29T08:45:56.681266Z","iopub.status.idle":"2024-02-29T08:45:56.706659Z","shell.execute_reply.started":"2024-02-29T08:45:56.681228Z","shell.execute_reply":"2024-02-29T08:45:56.705692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Indeed, there is a remarkable imbalance present in the data that we should take care of. If we would not deal with this problem we would actually train our model to be biased and to predict 'No Cancer' most of the time. We would obtain a model that cannot predict 'Cancer' in a reliable way. In fact, there are multiple ways to deal with imbalanced data and prevent biased model performance. A few of them are:\n\n* Resampling (Oversampling / Undersampling)\n* Class Weighting during model building\n* Focus other evaluation metrics\n\nSince the evaluation metric is given by the competition we cannot change that. For this project we will use the resampling approach.","metadata":{}},{"cell_type":"markdown","source":"## 4.2 Balancing the Dataset\n\nWhen balancing the dataset using resampling we have two choices. We could either perform oversampling by increasing the number of instances in the minority class or undersampling by reducing the number of instances in the majority class. Both options have their pros and cons. One thing to consider here is that undersampling typically reduces the risk of overfitting and focuses more on learning the minority class. All this is can be achieved at the cost of some lost information in the majority class. Now, our minority class represents the images where cancer is present and since the main interesent lies in detecting those positive cases we will apply undersampling on our data instead of oversampling.\n\nAn additional advantage that comes for free doing so, is the fact that we are safer when it comes to memory issues by dropping some of our samples. We will even reduce the training size down to 50,000 samples per label. Ultimately, to avoid potential ordering in the image files that could introduce bias into our model we randomly shuffle our final dataset.","metadata":{}},{"cell_type":"code","source":"# sample positive and negative images\nsample_size = 50000\ndf_train_neg = df_train[df_train['label'] == 0].sample(sample_size, random_state=42)\ndf_train_pos = df_train[df_train['label'] == 1].sample(sample_size, random_state=42)\n\n# create a new shuffeled training dataset\ndf_train_sample = shuffle(pd.concat([df_train_pos, df_train_neg], axis=0).reset_index(drop=True))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:56.708206Z","iopub.execute_input":"2024-02-29T08:45:56.708609Z","iopub.status.idle":"2024-02-29T08:45:56.761703Z","shell.execute_reply.started":"2024-02-29T08:45:56.708573Z","shell.execute_reply":"2024-02-29T08:45:56.760427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_label_dist(df_train_sample)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:56.763556Z","iopub.execute_input":"2024-02-29T08:45:56.764015Z","iopub.status.idle":"2024-02-29T08:45:57.019476Z","shell.execute_reply.started":"2024-02-29T08:45:56.763977Z","shell.execute_reply":"2024-02-29T08:45:57.018543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"calc_imbalance(df_train_sample)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.020721Z","iopub.execute_input":"2024-02-29T08:45:57.021048Z","iopub.status.idle":"2024-02-29T08:45:57.038125Z","shell.execute_reply.started":"2024-02-29T08:45:57.021017Z","shell.execute_reply":"2024-02-29T08:45:57.037047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As we can see, the resulting dataset is now balanced and contains a total of 100,000 images that we will use for model building. But before we can actually build our neural networks, we have to prepare our data appropriately since we will work with the PyTorch library.","metadata":{}},{"cell_type":"markdown","source":"## 4.3 Plan of Analysis\n\nBased on our EDA so far, it makes sense to use the undersampled (balanced) dataset during model building. Before we can actually start with model building there are a few necessary steps to perform.\n\n* Data augmentation\n* Normalization\n* Splitting into training and validation set\n\nData augmentation is a technique used to artificially increase the size of a training dataset by creating new examples through various transformations. These transformations can include rotations, flips, cropping, resizing, and changes in color intensity. This helps to improve the generalization and robustness of a model by exposing it to a broader range of variations present in real-world data.\n\nNormalization is another preprocessing step oftentimes performed in image-data related problems that involves scaling the pixel values to values on a smaller range (originally 0 to 255). This helps in stabilizing neural network training and can lead to faster and more accurate convergence of the underlying optimization algorithms.\n\nSplitting the data into training and validation set helps us estimate how well the trained model will generalize to previously unseen data. It can also avoid overfitting by stopping the training process early if the validation performance starts to get worse. ","metadata":{}},{"cell_type":"markdown","source":"### 4.3.1 Data augmentation and normalization\n\nAfter doing some research on the PyTorch library it becomes clear, that the data augmentation and normalization can be done using built-in features of this library. In order to avoid duplicated code (since we have to do similar things for the test data later on) we will create a custom class that wraps our dataframe in such a way that is can processed by PyTorch. We have to inherit from the 'Dataset' class and and are forced to implement the 'len' and 'getitem' methods. This is a quite similar to the Decorator pattern commonly used in software architectures.\n\nAn example can be seen here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html","metadata":{}},{"cell_type":"code","source":"# wrapper class for PyTorch dataset\nclass PyTorchData(Dataset):\n    \n    # set the necessary super class properties\n    def __init__(self, df, folder = './', transform=None):\n        super().__init__()\n        self.df = df.values\n        self.data_dir = folder\n        self.transform = transform\n\n    # returns the length of the dataset\n    def __len__(self):\n        return len(self.df)\n    \n    # returns the image with the given index and applies a transformation (if specified)\n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name+'.tif')\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.045129Z","iopub.execute_input":"2024-02-29T08:45:57.045451Z","iopub.status.idle":"2024-02-29T08:45:57.053983Z","shell.execute_reply.started":"2024-02-29T08:45:57.045424Z","shell.execute_reply":"2024-02-29T08:45:57.052902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now, we can define the transformations peformed during data augmentation. We will define some simple flips and rotations using the 'transforms' object from PyTorch library. After definition is done we can create a new object of our custom class by passing the transform object into the constructor.","metadata":{}},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(), \n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n])\n\ntrain_torch = PyTorchData(df_train_sample, folder_train, transform_train)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.055114Z","iopub.execute_input":"2024-02-29T08:45:57.055430Z","iopub.status.idle":"2024-02-29T08:45:57.075575Z","shell.execute_reply.started":"2024-02-29T08:45:57.055396Z","shell.execute_reply":"2024-02-29T08:45:57.074707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.2 Data splitting\n\nNow we will split our data into training and validation set. First, we determine the indices of the training and validation samples while using 15% of our data for validation. Then we pass them to another PyTorch object with the ability of random subset sampling. Finally we pass the previously created dataset, a batch size and the sampler to a so-called 'DataLoader'.\n\nThe 'DataLoader' allows us to access features and labels of our dataset on a per-sample basis. During model training, it's common to work with samples in minibatches, shuffle data per epoch to mitigate overfitting, and leverage Python's multiprocessing to accelerate data retrieval. The 'DataLoader' simplifies this process for us.","metadata":{}},{"cell_type":"code","source":"batch_size = 128\n\n# set training and validation indices \nindices = list(range(len(train_torch)))\nsplit = int(np.floor(0.15 * len(train_torch)))\ntrain_idx, valid_idx = indices[split:], indices[:split]\n\n# random samplers\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\n\n# prepare data loaders\ntrain_loader = DataLoader(train_torch, batch_size=batch_size, sampler=train_sampler)\nvalid_loader = DataLoader(train_torch, batch_size=batch_size, sampler=valid_sampler)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.077313Z","iopub.execute_input":"2024-02-29T08:45:57.077768Z","iopub.status.idle":"2024-02-29T08:45:57.089561Z","shell.execute_reply.started":"2024-02-29T08:45:57.077731Z","shell.execute_reply":"2024-02-29T08:45:57.088431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4.3.3 Preparing test data\n\nAfter setting up our training data, we perform similar steps to prepare our test data. Of course this time we do not need to split the data or apply any transformations like rotation or flipping. By using the sample submission CSV file we get access to all the image ID's while still having a dataframe format. ","metadata":{}},{"cell_type":"code","source":"transform_test = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5],std=[0.5, 0.5, 0.5])\n])\n\ntest_torch = PyTorchData(df_sample_sub, folder_test, transform_test)\ntest_loader = DataLoader(test_torch, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.091157Z","iopub.execute_input":"2024-02-29T08:45:57.092061Z","iopub.status.idle":"2024-02-29T08:45:57.104629Z","shell.execute_reply.started":"2024-02-29T08:45:57.092009Z","shell.execute_reply":"2024-02-29T08:45:57.103667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Model Building and Hyperparameter Tuning\n\n## 5.1 Basic CNN model\n\nFirst, we will check whether we can run our neural networks on a GPU. This will help to reduce computing time.","metadata":{}},{"cell_type":"code","source":"clear_memory()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.105929Z","iopub.execute_input":"2024-02-29T08:45:57.106237Z","iopub.status.idle":"2024-02-29T08:45:57.558367Z","shell.execute_reply.started":"2024-02-29T08:45:57.106198Z","shell.execute_reply":"2024-02-29T08:45:57.557140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(\"Device: \", device)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.559707Z","iopub.execute_input":"2024-02-29T08:45:57.560008Z","iopub.status.idle":"2024-02-29T08:45:57.599721Z","shell.execute_reply.started":"2024-02-29T08:45:57.559982Z","shell.execute_reply":"2024-02-29T08:45:57.598511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next, we define our network structure. The first layer captures the 32 pixels as required by the competition. The following architecture consists of several convolutional blocks with increasing numbers of channels, followed by a fully connected section for classification. Batch normalization and ReLU activation functions are used to enhance training stability and convergence. Max-pooling is used to reduce spatial dimensions, and dropout is applied to reduce overfitting.","metadata":{}},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        \n        self.conv1 = nn.Sequential(\n                        nn.Conv2d(3, 32, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(32),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv2 = nn.Sequential(\n                        nn.Conv2d(32, 64, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(64),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv3 = nn.Sequential(\n                        nn.Conv2d(64, 128, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(128),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv4 = nn.Sequential(\n                        nn.Conv2d(128, 256, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(256),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        self.conv5 = nn.Sequential(\n                        nn.Conv2d(256, 512, 3, stride=1, padding=1),\n                        nn.BatchNorm2d(512),\n                        nn.ReLU(inplace=True),\n                        nn.MaxPool2d(2,2))\n        \n        \n        self.fc=nn.Sequential(\n                nn.Linear(512*3*3, 256),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm1d(256),\n                nn.Dropout(0.4),\n                nn.Linear(256, 1))\n        \n    def forward(self,x):\n        x=self.conv1(x)\n        x=self.conv2(x)\n        x=self.conv3(x)\n        x=self.conv4(x)\n        x=self.conv5(x)\n        x=x.view(x.shape[0],-1)\n        x=self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.601220Z","iopub.execute_input":"2024-02-29T08:45:57.602253Z","iopub.status.idle":"2024-02-29T08:45:57.616922Z","shell.execute_reply.started":"2024-02-29T08:45:57.602210Z","shell.execute_reply":"2024-02-29T08:45:57.615624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create CNN\nmodel = CNN().to(device)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.618291Z","iopub.execute_input":"2024-02-29T08:45:57.618849Z","iopub.status.idle":"2024-02-29T08:45:57.893967Z","shell.execute_reply.started":"2024-02-29T08:45:57.618816Z","shell.execute_reply":"2024-02-29T08:45:57.892719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We perform a manual hyperparameter tuning on the learning rate. In order to stay within computation time limits, we will only look at the three different values listed below. For the same reason we will only iterate over 10 epochs. We use a binary cross-entropy loss function.","metadata":{}},{"cell_type":"code","source":"# hyperparameters to tune\nlearning_rates = [0.001, 0.0005, 0.0002]","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.895699Z","iopub.execute_input":"2024-02-29T08:45:57.896005Z","iopub.status.idle":"2024-02-29T08:45:57.900724Z","shell.execute_reply.started":"2024-02-29T08:45:57.895980Z","shell.execute_reply":"2024-02-29T08:45:57.899611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save final results\nresults = []\nbest_model_idx = None\nbest_auc = 0.0\nbest_model_cnn = None\n\n# iterate all values for the hyperparameter\nfor idx, lr in enumerate(learning_rates):\n    \n    # use GPU if available\n    model = CNN().to(device)\n    \n    # define optimizer and loss function\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.BCEWithLogitsLoss()\n\n    # save results over epochs \n    train_losses = []\n    valid_losses = []\n    valid_aucs = []   \n    n_epochs = 10\n\n    # iterate all epochs\n    for epoch in range(1, n_epochs+1):\n        \n        valid_aucs_epoch = [] \n        model.train()\n        train_loss = 0.0\n        \n        # process the training images\n        for data, target in train_loader:\n            optimizer.zero_grad()\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output.view(-1), target.float())\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * data.size(0)\n        \n        model.eval()\n        valid_loss = 0.0\n        \n        # process the validation images\n        for data, target in valid_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            loss = criterion(output.view(-1), target.float())\n            valid_loss += loss.item() * data.size(0)\n            y_actual = target.data.cpu().numpy()\n            y_pred = torch.sigmoid(output).detach().cpu().numpy()\n            valid_aucs_epoch.append(roc_auc_score(y_actual, y_pred))\n        \n        # determine values for current epoch\n        train_loss /= len(train_loader.sampler)\n        valid_loss /= len(valid_loader.sampler)\n        valid_auc = np.mean(valid_aucs_epoch)\n        \n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        valid_aucs.append(valid_auc)\n\n        print('Learning Rate: {:.6f} | Epoch: {} | Training Loss: {:.6f} | Validation Loss: {:.6f} | Validation AUC: {:.4f}'.format(lr, epoch, train_loss, valid_loss, valid_auc))\n    \n    # save results for current value of hyperparameter\n    results.append({'learning_rate': lr, 'train_losses': train_losses, 'valid_losses': valid_losses, 'valid_aucs': valid_aucs})\n    \n    # save best model\n    avg_auc = np.mean(valid_aucs)\n    if avg_auc > best_auc:\n        best_auc = avg_auc\n        best_model_idx = idx\n        best_model_cnn = model\n    \n# print best hyperparameter\nprint(\"Best learning rate according to hyperparameter search: \", learning_rates[best_model_idx])","metadata":{"execution":{"iopub.status.busy":"2024-02-29T08:45:57.902032Z","iopub.execute_input":"2024-02-29T08:45:57.902432Z","iopub.status.idle":"2024-02-29T10:51:18.330558Z","shell.execute_reply.started":"2024-02-29T08:45:57.902391Z","shell.execute_reply":"2024-02-29T10:51:18.329438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's plot the results.","metadata":{}},{"cell_type":"code","source":"# plot training and validation loss over epochs for best model\ndef plot_losses(train_losses, valid_losses, title):\n    plt.figure(figsize=(10, 5))\n    plt.plot(train_losses, label=\"Train Loss\")\n    plt.plot(valid_losses, label=\"Valid Loss\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title(title)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:18.331915Z","iopub.execute_input":"2024-02-29T10:51:18.332252Z","iopub.status.idle":"2024-02-29T10:51:18.339117Z","shell.execute_reply.started":"2024-02-29T10:51:18.332225Z","shell.execute_reply":"2024-02-29T10:51:18.337960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_result = results[best_model_idx]\nplot_losses(best_result['train_losses'], best_result['valid_losses'], 'Training and Validation Loss over Epochs (best model)')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:18.340737Z","iopub.execute_input":"2024-02-29T10:51:18.341139Z","iopub.status.idle":"2024-02-29T10:51:18.651100Z","shell.execute_reply.started":"2024-02-29T10:51:18.341103Z","shell.execute_reply":"2024-02-29T10:51:18.650003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot validation auc over epochs for best model\ndef plot_aucs(aucs, title):\n    plt.figure(figsize=(10, 5))\n    plt.plot(aucs, label=\"Validation AUC\")\n    plt.xlabel('Epochs')\n    plt.ylabel('Validation AUC')\n    plt.title(title)\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:18.652243Z","iopub.execute_input":"2024-02-29T10:51:18.652574Z","iopub.status.idle":"2024-02-29T10:51:18.659298Z","shell.execute_reply.started":"2024-02-29T10:51:18.652548Z","shell.execute_reply":"2024-02-29T10:51:18.658062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_result = results[best_model_idx]\nplot_aucs(best_result['valid_aucs'], 'Validation AUC over Epochs (best model)')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:18.660796Z","iopub.execute_input":"2024-02-29T10:51:18.661180Z","iopub.status.idle":"2024-02-29T10:51:18.978223Z","shell.execute_reply.started":"2024-02-29T10:51:18.661144Z","shell.execute_reply":"2024-02-29T10:51:18.977192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The results seem to be reasonable and pretty good. We will discuss the results in more detail in the next section. But before, we will try out another model architecture.","metadata":{}},{"cell_type":"markdown","source":"## 5.2 Dense (Pretrained) Network\n\nFor our second model, we do not perform a hyperparameter search again since the whole notebook would need too much GPU time. Therefore, we reuse the previously found best learning rate of our first CNN architecture. For the same reason we also perform only 10 epochs as with our first model. And the same as before, we use a binary cross-entropy loss function.\n\nThe main difference will be that we now build our architecture upon a given pretrained dense network architecture where each layer is connected to every other layer in a feed-forward fashion. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several advantages compared to standard CNNs. They mitigate the vanishing-gradient problem, and reduce the number of parameters to name just a few.","metadata":{}},{"cell_type":"code","source":"clear_memory()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:18.980600Z","iopub.execute_input":"2024-02-29T10:51:18.981034Z","iopub.status.idle":"2024-02-29T10:51:19.330111Z","shell.execute_reply.started":"2024-02-29T10:51:18.980997Z","shell.execute_reply":"2024-02-29T10:51:19.328983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DenseNetModified(nn.Module):\n    def __init__(self):\n        super(DenseNetModified, self).__init__()\n        \n        # use a pretrained dense net architecture\n        self.densenet = models.densenet121(pretrained=True)\n        \n        num_features = self.densenet.classifier.in_features\n        self.densenet.classifier = nn.Sequential(\n            nn.Linear(num_features, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.5),\n            nn.Linear(512, 1)\n        )\n        \n    def forward(self, x):\n        return self.densenet(x)","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:19.332305Z","iopub.execute_input":"2024-02-29T10:51:19.332702Z","iopub.status.idle":"2024-02-29T10:51:19.341317Z","shell.execute_reply.started":"2024-02-29T10:51:19.332663Z","shell.execute_reply":"2024-02-29T10:51:19.340280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reuse best learning rate from first CNN\nlr = learning_rates[best_model_idx]\nmodel_dense = DenseNetModified().to(device)\n\n# set optimizer and loss function\noptimizer = optim.Adam(model_dense.parameters(), lr=lr)\ncriterion = nn.BCEWithLogitsLoss()\n\n# store final results\ntrain_losses_dense = []\nvalid_losses_dense = []\nvalid_aucs_dense = []\nn_epochs = 10\n\n# iterate all epochs\nfor epoch in range(1, n_epochs+1):\n\n    valid_aucs_epoch = [] \n    model_dense.train()\n    train_loss = 0.0\n \n    # process training images\n    for data, target in train_loader:\n        optimizer.zero_grad()\n        data, target = data.to(device), target.to(device)\n        output = model_dense(data)\n        loss = criterion(output.view(-1), target.float())\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() * data.size(0)\n\n    model_dense.eval()\n    valid_loss = 0.0\n\n    # process validation images\n    for data, target in valid_loader:\n        data, target = data.to(device), target.to(device)\n        output = model_dense(data)\n        loss = criterion(output.view(-1), target.float())\n        valid_loss += loss.item() * data.size(0)\n        y_actual = target.data.cpu().numpy()\n        y_pred = torch.sigmoid(output).detach().cpu().numpy()\n        valid_aucs_epoch.append(roc_auc_score(y_actual, y_pred))\n\n    # store final results\n    train_loss /= len(train_loader.sampler)\n    valid_loss /= len(valid_loader.sampler)\n    valid_auc = np.mean(valid_aucs_epoch)\n\n    train_losses_dense.append(train_loss)\n    valid_losses_dense.append(valid_loss)\n    valid_aucs_dense.append(valid_auc)\n\n    print('Learning Rate: {:.6f} | Epoch: {} | Training Loss: {:.6f} | Validation Loss: {:.6f} | Validation AUC: {:.4f}'.format(lr, epoch, train_loss, valid_loss, valid_auc))","metadata":{"execution":{"iopub.status.busy":"2024-02-29T10:51:19.342528Z","iopub.execute_input":"2024-02-29T10:51:19.342890Z","iopub.status.idle":"2024-02-29T11:40:28.910775Z","shell.execute_reply.started":"2024-02-29T10:51:19.342863Z","shell.execute_reply":"2024-02-29T11:40:28.909695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses_dense, valid_losses_dense, 'Training and Validation Loss over Epochs')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:28.912190Z","iopub.execute_input":"2024-02-29T11:40:28.912524Z","iopub.status.idle":"2024-02-29T11:40:29.206173Z","shell.execute_reply.started":"2024-02-29T11:40:28.912491Z","shell.execute_reply":"2024-02-29T11:40:29.204975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_aucs(valid_aucs_dense, 'Validation AUC over Epochs')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:29.207518Z","iopub.execute_input":"2024-02-29T11:40:29.207974Z","iopub.status.idle":"2024-02-29T11:40:29.480323Z","shell.execute_reply.started":"2024-02-29T11:40:29.207937Z","shell.execute_reply":"2024-02-29T11:40:29.479267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Again the results seem to be quite good. Let's dive into the last section where we discuss the results in detail and compare our two model architectures.","metadata":{}},{"cell_type":"markdown","source":"# 6. Results and Analysis\n\n## 6.1 Model summary and comparison\n\nAs we have already seen both of our models seem to work quite well on the training data. Now, it's time to compare them and select the better one for predicting on the test data. First, lets have a closer look at the training and validation losses.\n\nAs a reminder, we did a hyperparameter tuning on the learning rate for the first model where we tried out three different values. The best one selected was the following.","metadata":{}},{"cell_type":"code","source":"print(\"Best learning rate according to hyperparameter search on Classic CNN model: \", learning_rates[best_model_idx])","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:29.481915Z","iopub.execute_input":"2024-02-29T11:40:29.482822Z","iopub.status.idle":"2024-02-29T11:40:29.490166Z","shell.execute_reply.started":"2024-02-29T11:40:29.482782Z","shell.execute_reply":"2024-02-29T11:40:29.487427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to stay within computation time limits we reused this value for the second dense net model. Now, let's compare the losses of our two models.","metadata":{}},{"cell_type":"code","source":"plot_losses(best_result['train_losses'], best_result['valid_losses'], 'Training and Validation Loss over Epochs - Classic CNN')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:29.491728Z","iopub.execute_input":"2024-02-29T11:40:29.492115Z","iopub.status.idle":"2024-02-29T11:40:29.854698Z","shell.execute_reply.started":"2024-02-29T11:40:29.492079Z","shell.execute_reply":"2024-02-29T11:40:29.853641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_losses(train_losses_dense, valid_losses_dense, 'Training and Validation Loss over Epochs - Dense Net')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:29.856164Z","iopub.execute_input":"2024-02-29T11:40:29.857182Z","iopub.status.idle":"2024-02-29T11:40:30.207186Z","shell.execute_reply.started":"2024-02-29T11:40:29.857143Z","shell.execute_reply":"2024-02-29T11:40:30.206103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first plot shows the classic CNN architecture. We observe a decreasing training loss as the epoch number increases which is what we would expect. The validation loss looks more unsteady but overall also decreases when reaching the higher epoch numbers. As already mentioned, we could potentially improve here if we would allow for more epochs. But since there is time limit on Kaggle for using the GPU we restrict ourselves a little bit for this project.\n\nThe second architecture clearly shows better results. Already after the first epoch there is remarkable difference in training loss, which means the dense network architecture works much better. The same can be said about higher epochs. We constantly get smaller loss values. The reason for the better performance is probably the fact that the connected layer structure of the network can capture more complex patterns in the dataset as compared to the classic CNN structure. Also the fact that the dense network is pretrained gives the model a headstart and helps to optimize faster.\n\nLet's see if our dense model also outperforms when looking at the AUC scores.","metadata":{}},{"cell_type":"code","source":"plot_aucs(best_result['valid_aucs'], 'Validation AUC over Epochs - Classic CNN')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:30.208567Z","iopub.execute_input":"2024-02-29T11:40:30.208925Z","iopub.status.idle":"2024-02-29T11:40:30.465277Z","shell.execute_reply.started":"2024-02-29T11:40:30.208898Z","shell.execute_reply":"2024-02-29T11:40:30.464157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_aucs(valid_aucs_dense, 'Validation AUC over Epochs - Dense Net')","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:30.466812Z","iopub.execute_input":"2024-02-29T11:40:30.467160Z","iopub.status.idle":"2024-02-29T11:40:30.811241Z","shell.execute_reply.started":"2024-02-29T11:40:30.467131Z","shell.execute_reply":"2024-02-29T11:40:30.810175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now looking at the AUC scores we can basically conclude the same. The first model shows good results and an increasing score over the epochs. Similar to the losses the dense structure starts out at better values than the classic CNN. Even if the increase over epochs is not as significant as with the first model, the absolute values are still slightly higher.\n\nSummarizing the final results everything seems to be pretty easy. But of course, there were some problems during model building and training, e.g. computing time limits, understanding model architectures, and debugging the training process. We will talk about these topics later on in the Learnings and Takeaways section.","metadata":{}},{"cell_type":"markdown","source":"## 6.2 Predicting on test data\n\nWe have seen that the dense model performed the best. We will use it now to predict on the test data and create our submission file.","metadata":{}},{"cell_type":"code","source":"clear_memory()","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:40:30.812543Z","iopub.execute_input":"2024-02-29T11:40:30.812962Z","iopub.status.idle":"2024-02-29T11:40:31.310225Z","shell.execute_reply.started":"2024-02-29T11:40:30.812932Z","shell.execute_reply":"2024-02-29T11:40:31.309054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# turn of gradients\nmodel_dense.eval()\npreds = []\n\n# iterate all test images\nfor batch_i, (data, target) in enumerate(test_loader):\n    data, target = data.to(device), target.to(device)\n    output = model_dense(data)\n\n    pr = output.detach().cpu().numpy()\n    for i in pr:\n        preds.append(i)\n\n# add predicted labels to submission file    \ndf_sample_sub['label'] = preds","metadata":{"execution":{"iopub.status.busy":"2024-02-29T11:55:01.978206Z","iopub.execute_input":"2024-02-29T11:55:01.978588Z","iopub.status.idle":"2024-02-29T11:56:56.243961Z","shell.execute_reply.started":"2024-02-29T11:55:01.978557Z","shell.execute_reply":"2024-02-29T11:56:56.242702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert probabilities to float\nfor i in range(len(df_sample_sub)):\n    df_sample_sub.label[i] = float(df_sample_sub.label[i]) ","metadata":{"execution":{"iopub.status.busy":"2024-02-29T13:09:48.503720Z","iopub.execute_input":"2024-02-29T13:09:48.504133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create submission file\ndf_sample_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Conclusion\n\n## 7.1 Result Summary\n\nAs we have already seen in the previous section, the dense network performed better than the CNN network because of its ability to capture more complex structures in our images. However, both models did a good job on the training data. Looking at the performance of the dense network on the test data we saw that we obtain a really good final score.","metadata":{}},{"cell_type":"markdown","source":"## 7.2 Learnings and Takeaways\n\nWorking with neural networks requires a different style of work compared to simpler supervised learning techniques like regression analysis. Getting results from model training might last several hours. Therefore, it is really important to work clean and precise so that you avoid having many training iterations. One aspect that might be underestimated but helped me a lot during this project is the usage of print statements in the code. This way you can debug your code better, inspect intermediate results and find errors more easily. Besides that aspect, image data in particalur requires quite different preprocessing steps compared to e.g. textual data. Techniques like image augmentation can really improve your model if used correctly.\n\nAnother problem I encountered during my work was the memory limitations of the GPU on Kaggle. On way to solve this issue was to reduce the batch size from 256 down to 128 which leads to less RAM consumptions during training and evaluating. Additionaly, I had to call the garbage collector from time to time in order to free unused memory.","metadata":{}},{"cell_type":"markdown","source":"## 7.3 What didn't work\n\nOverall, everything worked quite well. However, it would have been great to work with more epochs in order to get more precise results. The limited GPU memory really was a problem in this project since image data requires so much disk space. It took quite a few iterations to find suitable values for the number of included images, the batch size or number of epochs while still getting meaningful results. ","metadata":{}},{"cell_type":"markdown","source":"## 7.4 Possible improvements\n\nOne possible improvement would be to include the whole image set instead of just a smaller amount due to memory issues. Also using more epochs would possibly enhance model performance. It might be also interesting to see how much better the model would perform with a balanced image set compared to the original unbalanced one.\n\nIf we had enough computing power, we could also expand our hyperparameter tuning by including more parameters and a wider value range.","metadata":{}}]}